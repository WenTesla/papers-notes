# 非结构化日志中的自注意Classification-Based异常检测-Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs



## 背景

**现有方法不足**：主流单类深度学习方法（如基于 LSTM 的 DeepLog）依赖人工指定日志表示，存在两方面局限：一是无法学习区分**正常与异常日志语义差异**的表示，对未见过的日志泛化差；二是**依赖日志解析将日志转为模板**，易因解析不完美导致向量表示缺陷，且预训练词向量（如基于 Wikipedia）与系统日志语言域差异大

## 前提

正常日志消息应该具有彼此之间距离较近的向量表示，例如集中在一个紧密的球体内，并且异常应该远离正常样本的分布。这一假设是许多异常检测方法设计的基础，其核心逻辑是通过向量空间中样本的聚集程度区分正常与异常。

## 挑战

难以获取兼具代表性与紧凑性的日志数值嵌入向量

## 解决方法

核心思路是**通过分类任务学习日志向量表示**：训练神经网络将目标系统（需进行异常检测的系统）的正常日志数据，与其他系统的辅助日志数据集（作为异常样本的替代）进行区分。

这种设计的核心优势在于，辅助数据集既能为正常数据的表示学习提供更丰富信息，又能通过引入多样性起到正则化作用，避免模型过拟合，最终提升对未见过日志的泛化能力。

## 设计

![img](https://cdn.xljsci.com/literature/177852894/page3/10vhz1.png)

**日志预处理（分词）**：用 NLTK 库处理原始日志，过滤 HTTP / 系统路径、小写转换、移除特殊字符与含数字的变量 token、剔除英文停用词，最后添加特殊 “[EMBEDDING]”token（用于汇总日志上下文语义），无需日志解析步骤，避免信息丢失。

**模型架构与目标函数**：

- **架构**：采用带多头自注意力的 Transformer 编码器，将分词后的日志 token 转为向量表示，通过 “[EMBEDDING]”token 输出日志的整体向量（z）。
- **目标函数**：提出超球面分类损失函数，将正常日志（目标系统数据，标签 0）约束在超球面中心附近（向量紧凑），异常日志（辅助数据集，标签 1）远离中心；用高斯径向基函数计算异常得分，距离中心越远得分越高，通过阈值判断是否为异常。

**\(D \cup A\)**：代表训练数据的整体，**由 “目标系统日志集合 D” 与 “辅助日志数据集 A” 拼接而成**，其中每个元素 \((S_{i}, y_{i})\) 包含单条日志的向量表示 \(S_{i}\) 与标签 \(y_{i}\)

- \(S_{i} \in \mathbb{R}^{d \times|r_{i}|}\)：表示第i条日志的向量形式，d是 token 嵌入的维度，\(|r_{i}|\)是该日志经分词后的 token 数量，即日志向量为d行、\(|r_{i}|\)列的矩阵。
- \(y_{i} \in \{0,1\}\)：是日志样本的标签，**\(y_{i}=0\)对应目标系统的正常日志，\(y_{i}=1\)对应辅助数据集的异常日志（辅助数据在此处被用作异常样本的替代）**。

**编码器函数 \(\phi(S_{i}, \theta)\)**：是模型的核心映射组件，参数为\(\theta\)，作用是将日志的 token 嵌入向量（维度d）转化为维度为p的向量，即完成从原始日志表示到紧凑特征向量的提取。

**异常得分函数 \(l(·)\)**：**接收编码器输出的p维向量，将其映射到\([0,1]\)区间内的数值**



考虑到目标函数的假设会增强表示的紧凑性，即样本靠近球体\(c=0\)的中心，我们将异常分数定义为（从“EMBEDDING”令牌获得的）日志向量到超球面中心c的距离。 \[A\left(x_{i}\right)=\left\| \phi\left(x_{i} ; \theta\right)\right\| ^{2}\]

标准的二叉交叉损失熵如下（改进前）
$$
loss=-\frac{1}{n} \sum_{i=1}^{n}\left(1-y_{i}\right) log l\left(\phi\left(S_{i} ; \theta\right)\right)+y_{i} log \left(1-l\left(\phi\left(S_{i} ; \theta\right)\right)\right)
$$

- 当\(y_i=0\)（正常样本-系统正常日志）：\(1-y_i=1\)，该项简化为\(-\log l(z)\)。模型需最小化损失，因此需让\(l(z)\)尽可能大（接近 1），即正常样本的异常得分需高；
- 当\(y_i=1\)（异常样本）：\(y_i=1\)，该项简化为\(\log (1-l(z))\)。模型需最小化损失，因此需让\(l(z)\)尽可能小（接近 0），即异常样本的异常得分需低。

为了实现超球面，改进了异常得分函数，通过高斯径向基函数的特性，将 “得分约束” 转化为 “距离约束”，强制正常向量紧凑。
$$
l(z)=exp(-\|z\|^2)
$$
**距离越远，函数值越小。距离越大，函数值越大**

- z：编码器输出的日志向量（维度为p），是模型对日志的核心特征表示；
- \(\|z\|\)：向量z的**欧几里得范数**（即向量到空间原点\(c=0\)的直线距离），\(\|z\|^2\)是距离的平方；
- 以指数形式将 “向量到原点的距离” 映射为 [0,1] 区间的数值（即新的异常得分）

> l是单调递减函数，Z越大，l越小

将函数带入，得到超球面分类器
$$
loss=\frac{1}{n} \sum_{i=1}^{n}\left(1-y_{i}\right)\left\| \phi\left(S_{i} ; \theta\right)\right\| ^{2}-y_ilog(1-exp(−‖φ(Si; θ)‖^2))
$$
拓展，当y_i=0的时候，这个时候是正常样本。
$$
loss=\frac1n\sum_{i=1}^n\left\| \phi\left(S_{i} ; \theta\right)\right\| ^{2}\
$$
当y_i=1的时候，这个是异常样本
$$
loss=-log(1-exp(−‖φ(Si; θ)‖^2))
$$


sigmoid 函数是 “用超平面划区域”，高斯径向基函数是 “用原点距离定疏密”—— 通过后者，Logsy 将原本无约束的 “半空间决策”，转化为有明确中心的 “距离约束决策”.

模型。Logsy有两种运行模式——离线模式和在线模式。在离线阶段，日志消息用于通过反向传播调整所有模型参数，并选择最佳超参数。在在线阶段，每条日志消息都会通过保存的模型进行前向传递。这会为每条消息生成相应的日志向量表示z和异常分数。

## 结果

![img](https://cdn.xljsci.com/literature/177852894/page5/2vhzpr.png)

![img](https://cdn.xljsci.com/literature/177852894/page5/mqswvw.png)