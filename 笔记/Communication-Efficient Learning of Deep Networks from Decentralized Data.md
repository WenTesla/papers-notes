# Communication-Efficient Learning of Deep Networks from Decentralized Data-基于分散数据的深度网络通信高效学习（联邦学习的开山之作）

## 摘要

我们提出了一种**基于迭代模型平均的深度网络联合学习的实用方法**，并进行了广泛的经验评估，考虑了五种不同的模型架构和四种数据集。这些实验表明，该方法对不平衡和非iid数据分布具有鲁棒性，这是该设置的定义特征。通信成本是主要约束，与同步随机梯度下降相比，我们显示所需的通信回合减少了10 - 100倍。

> non-IID（non-independent and identically distributed）指非独立同分布



![image-20250422200107885](https://image-bed-1313520634.cos.ap-beijing.myqcloud.com/image-20250422200107885.png)

## 背景

现代移动设备拥有大量适合训练模型的数据，但这些数据具有隐私敏感、数量大的特点，传统将数据上传至数据中心训练的方式存在风险

## **联邦学习的优势与优化问题特性**

**隐私优势**：与在数据中心训练持久化数据相比，联邦学习传输的是模型更新，信息最少且可短暂存在，还可通过匿名网络传输，降低隐私风险。

**优化问题特性**：联邦优化问题具有**非 IID（客户端数据不代表总体分布）**、**不平衡（客户端数据量不同）**、**大规模分布（客户端数量多）、通信受限（移动设备网络条件差）**的特点。



我们假设在通信轮中进行同步更新方案。有一组固定的K个客户端，每个客户端都有一个固定的本地数据集。在每轮开始时，选择客户端的随机分数C，服务器将当前全局算法状态发送给这些客户端的每个客户端（例如，当前模型参数）。为了提高效率，我们只选择一小部分客户，因为我们的实验表明，超过某一点后，增加更多客户的回报会递减。然后，每个选定的客户机根据全局状态及其本地数据集执行本地计算，并向服务器发送更新。然后，服务器将这些更新应用于其全局状态，并重复该过程。

<img src="https://image-bed-1313520634.cos.ap-beijing.myqcloud.com/image-20250423105232104.png" alt="image-20250423105232104" style="zoom:67%;" />



![img](https://oss.xljsci.com//literature/154170420/page0/1745382590186.png)

